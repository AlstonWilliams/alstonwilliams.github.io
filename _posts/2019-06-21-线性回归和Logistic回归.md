---
layout: post
title: 线性回归和Logistic回归
date: 2019-06-21
author: AlstonWilliams
header-img: img/post-bg-2015.jpg
catalog: true
categories:
- 机器学习相关
tags:
- 机器学习相关
---

## 线性回归

#### 普通线性回归

普通线性回归很简单，最简单的就是最小二乘法的矩阵求解形式。关于最小二乘法的矩阵求解形式，请查看[最小二乘法（least sqaure method）](https://zhuanlan.zhihu.com/p/38128785).

它求解的形式如下:
![](https://alstonwilliams.github.io/img/equation.svg)

我们可以看到，它有几个局限性:
- 对非线性的数据不友好，简单的多项式并不能求解
- 如果(X.T X)的逆并不存在，那么是无法解的
- 如果特征很多，那么求解会很费时间

还有另外一种非常常用的求解线性回归的方式，就是梯度下降法。详情请参考[机器学习笔记（三） 线性回归及梯度下降算法](https://blog.csdn.net/sinat_22594309/article/details/55203609)

#### 局部加权线性回归

局部加权线性回归就是为了解决`对非线性的数据不友好，简单的多项式并不能求解`这个问题，它每次只是取局部的数据做运算，使用高斯核来计算每个点和待预测点的相关性，然后求解最相关的几个点的损失函数的最小值，来求解。其实跟K近邻算法蛮像的。

详细介绍请看[详解局部加权回归](https://blog.csdn.net/Allenalex/article/details/16370245)

它求解的形式如下:
![](https://alstonwilliams.github.io/img/LWLR回归系数.png)

我们可以看到，尽管它可以克服`对非线性的数据不友好，简单的多项式并不能求解`，但是仍然对`如果(X.T X)的逆并不存在，那么是无法解的`束手无策.

#### 岭回归

岭回归的出现，就是为了解决`如果(X.T X)的逆并不存在，那么是无法解的`。而什么情况下逆会不存在呢，就是矩阵奇异的时候。所以岭回归的核心思想就是让矩阵非奇异，进而可以求逆。

它求解的形式如下:
![](https://alstonwilliams.github.io/img/岭回归回归系数.png)

## Logistic回归

Logistic回归和线性回归不同，Logistic回归是用于预测分类，结果是离散型的。而线性回归是回归问题，结果是连续型的。

Logistic回归的思路也很简单:
- 先给每个特征一个权重
- 求解梯度，更新权重

公式推导如下:
[Logistic Regression——逻辑回归算法推导](https://blog.csdn.net/Tomxiaodai/article/details/81748795)
